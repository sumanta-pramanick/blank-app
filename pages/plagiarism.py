import streamlit as st
from bs4 import BeautifulSoup
import requests
from PyPDF2 import PdfReader
from io import BytesIO
import plotly.graph_objects as go
import re
from threading import Thread
from streamlit.runtime.scriptrunner import add_script_run_ctx
import nltk
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
from sentence_transformers import SentenceTransformer, util
from navigation import make_sidebar
import openai


nltk.download("stopwords")
stop_words = set(stopwords.words("english"))
model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
openai.api_key = st.secrets["OPENAI_API_KEY"]


def detect_ai_text(text):
    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0.0,
        messages=[
            {
                "role": "system",
                "content": f"You are an intelligent bot, you responsibility is to check if the content is generated by AI or not.\nIf it's generated by AI then what is the percentage of the content that is generated by AI.\nthe response will be only the percentage and it should be between 0 to 100.\n\n",
            },
            {
                "role": "user",
                "content": f"Content to check:\n\n{text}\n\n",
            },
        ],
    )

    ai_percentage = float(response.choices[0].message.content)
    return ai_percentage


def extract_text_from_file(file):
    file_bytes = file.read()
    if file.type == "application/pdf":
        reader = PdfReader(BytesIO(file_bytes))
        text = ""
        for page in reader.pages:
            text += page.extract_text()
    else:
        text = file_bytes.decode("utf-8")
    return text


def split_text_into_sentences(text):
    return sent_tokenize(text)
    # chunk_size = 250
    # sentences = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]
    # return sentences


def search_query(query):
    base_url = "https://www.google.com/search?q="
    query = query.replace(" ", "+")
    url = base_url + query
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")
    divs = soup.find_all("div", class_="yuRUbf")
    urls = [div.find("a")["href"] for div in divs if div.find("a")]
    return urls[:1]


def fetch_text_from_url(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        text = " ".join(map(lambda p: p.text, soup.find_all(text=True)))
        return text
    except:
        return ""


def calculate_semantic_similarity(sentence1, sentence2):
    embeddings1 = model.encode([sentence1], convert_to_tensor=True)
    embeddings2 = model.encode([sentence2], convert_to_tensor=True)
    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)
    return cosine_scores.item()


def highlight_similar_text(extracted_text, similar_texts):
    similar_text = " ".join(similar_texts)

    pattern = r"\b(" + "|".join(map(re.escape, similar_text.split())) + r")+[\s\?\!]+\b"
    # st.write(pattern)
    matches = [
        (match.start(), match.end()) for match in re.finditer(pattern, extracted_text)
    ]

    combined_matches = []
    for start, end in matches:
        if combined_matches and combined_matches[-1][1] >= start - 1:
            combined_matches[-1] = (combined_matches[-1][0], end)
        else:
            combined_matches.append((start, end))

    highlighted_text = []
    last_end = 0
    for start, end in combined_matches:
        highlighted_text.append(extracted_text[last_end:start])
        highlighted_text.append(
            f"<span style='background-color: #90ee90'>{extracted_text[start:end]}</span>"
        )
        last_end = end
    highlighted_text.append(extracted_text[last_end:])

    return "".join(highlighted_text)


def display_plagiarism_results(results, extracted_text, unique_words):
    similarity_weight = 0.0
    word_weight = 1
    max_similarity = 0

    for result in results:
        searched_words = unique(result["searched_text"].split())
        # searched_words = [w for w in searched_words if not w.lower() in stop_words]

        # run through the unique_words and find how may sequence can be found from searched_words where number of words in a sequence is 3
        sequence_count = 0
        sequence_length = 5
        for i in range(len(unique_words) - sequence_length + 1):
            sequence = unique_words[i : i + sequence_length]
            if all(word in searched_words for word in sequence):
                sequence_count += 1
        if sequence_count > 5:
            weighted_similarity = (result["similarity"] * similarity_weight) + (
                len(result["matched_words"]) / len(searched_words)
            ) * 100 * word_weight
            # if weighted_similarity > max_similarity:
            max_similarity += weighted_similarity

    max_similarity = round(max_similarity / len(results))

    # Getting AI Generated Text Percentage
    ai_percentage = detect_ai_text(extracted_text)

    col1, col2 = st.columns(2)
    with col1:
        with st.container(border=True, height=600):
            highlighted_text = highlight_similar_text(extracted_text, unique_words)
            st.write(
                f"<div style='height: 100%;'>{highlighted_text}</div>",
                unsafe_allow_html=True,
            )

    with col2:
        with st.container(border=True, height=600):
            sub_col1, sub_col2 = st.columns(2)
            with sub_col1:
                st.plotly_chart(
                    create_circular_progress_bar(
                        max_similarity, "Plagiarised", "#FF5733"
                    ),
                    use_container_width=True,
                )
            with sub_col2:
                st.plotly_chart(
                    create_circular_progress_bar(
                        ai_percentage, "AI Generated", "#FF0000"
                    ),
                    use_container_width=True,
                )

            create_bar_graph(results)


def create_bar_graph(results):
    # Sample data
    values = ["30%", "80%", "50%"]

    for result in results:
        url = result["url"]
        value = result["similarity"]
        if value >= 71:
            color = "#FF0000"
        elif value >= 41:
            color = "#ffc816"
        else:
            color = "#50C878"
        st.write(
            f"""
            <div style="width: 100%; overflow: hidden; text-overflow: ellipsis; white-space: nowrap;">
                <a href="{url}" target="_blank">{url}</a>
            </div>
            <div style="display: flex; align-items: center;">
                <div style="width: {value}%; height: 20px; border-radius: 5%; background-color: {color}; color: white; text-align: center; line-height: 20px; font-size: 14px; margin-bottom: 10px;">{value}%</div>
            </div>
        """,
            unsafe_allow_html=True,
        )


def create_circular_progress_bar(value, title, color):
    fig = go.Figure(
        go.Indicator(
            mode="gauge+number",
            value=value,
            title={"text": title},  # Set position to bottom
            gauge={
                "axis": {"range": [0, 100]},
                "bar": {"color": color},
                "steps": [
                    {"range": [0, 50], "color": "lightgray"},
                    {"range": [50, 100], "color": "gray"},
                ],
                "threshold": {
                    "line": {"color": "red", "width": 4},
                    "thickness": 0.75,
                    "value": value,
                },
            },
        )
    )

    fig.update_layout(
        height=180,
        margin={"t": 0, "b": 0, "l": 0, "r": 0},
    )
    return fig


def unique(words):
    unique_words = []
    for word in words:
        if word not in unique_words:
            unique_words.append(word)
    return unique_words


def matched_word_lookup(extracted_text, fetched_text):
    extracted_words = extracted_text.split()
    # st.write(f"""extracted_text Length: {len(extracted_words)}""")
    fetched_words = fetched_text.split()
    # st.write(f"""fetched_words Length: {len(fetched_words)}""")
    # st.write(fetched_text)
    # st.write(fetched_words)
    # filtered_extracted_words = [
    #     w for w in extracted_words if not w.lower() in stop_words
    # ]
    # filtered_fetched_words = [w for w in fetched_words if not w.lower() in stop_words]
    # return [word for word in filtered_extracted_words if word in filtered_fetched_words]
    return [word for word in extracted_words if word in fetched_words]


def process_sentence(extracted_text, sentence, results):
    search_results = search_query(sentence)
    for url in search_results:
        fetched_text = fetch_text_from_url(url)
        if fetched_text:
            similarity = calculate_semantic_similarity(sentence, fetched_text)
            matched_words = matched_word_lookup(sentence, fetched_text)
            if similarity > 0.3 and len(matched_words) > 0:
                results.append(
                    {
                        "url": url,
                        "similarity": round(similarity * 100),
                        "matched_words": unique(matched_words),
                        "searched_text": sentence,
                        "fetched_text": fetched_text,
                    }
                )
                # st.write(f"Processing Completed for: {sentence}")
                # st.write(f"Similarity: {similarity}")
                # st.write(f"Matched Length: {len(matched_words)}")
                # st.write(f"Matched Words: {matched_words}")
                # st.write(f"URL: {url}")
                # st.write(f"Fetched Text: {fetched_text}")
                # st.write(f"----------------------------------------------------\n")


def check_plagiarism(extracted_text):
    try:
        sentences = split_text_into_sentences(extracted_text)
        # st.write(f"Total Sentences: {len(sentences)}")
        # st.write(sentences)

        results = []
        threads = []
        unique_results = []

        for sentence in sentences:
            if len(sentence.split()) <= 3:
                continue
            thread = Thread(
                target=process_sentence, args=(extracted_text, sentence, results)
            )
            ctx = add_script_run_ctx(thread)
            ctx.start()
            threads.append(ctx)

        for thread in threads:
            thread.join()

        if len(results) > 0:
            # get all matched words
            unique_words = []
            for result in results:
                unique_words.extend(result["matched_words"])
            unique_words = unique(unique_words)

            # order results by similarity descending
            results = sorted(results, key=lambda x: x["similarity"], reverse=False)

            # filter unique results by URL
            results = {result["url"]: result for result in results}.values()

            # order results by similarity descending
            unique_results = sorted(
                results, key=lambda x: x["similarity"], reverse=True
            )

            display_plagiarism_results(unique_results, extracted_text, unique_words)
        else:
            st.error("Experiencing issues with parsing data")
    except Exception as e:
        st.error(f"Error during plagiarism check: {e}")


def main():
    st.set_page_config(page_title="Plagiarism Detection", page_icon="🔍")
    make_sidebar()
    st.title("📝 Plagiarism Detection Tool")

    style = """
        <style>
            button{
                float:right;
            }

            a:link {
                color: green;
                text-decoration: none
            }

            a:visited {
                color: green;
            }

            a:hover {
                color: lightgreen;
            }

            a:active {
                color: green;
            }
        </style>
    """
    st.markdown(style, unsafe_allow_html=True)

    tab1, tab2 = st.tabs(["Enter Text", "Upload File"])

    with tab1:
        with st.container(border=True):
            txt = st.text_area(
                "Enter or paste the text to check for plagiarism", height=200
            )
            if st.button("🔍 Check text for Plagiarism"):
                if txt == "":
                    st.error("Please enter some text to check for plagiarism.")
                else:
                    with st.spinner("Analyzing... This might take a few minutes."):
                        check_plagiarism(txt)
    with tab2:
        with st.container(border=True):
            file = st.file_uploader("Upload Text or PDF File", type=["txt", "pdf"])
            if file:
                if st.button("🔍 Check file for Plagiarism"):
                    with st.spinner("Analyzing... This might take a few minutes."):
                        extracted_text = extract_text_from_file(file)
                        check_plagiarism(extracted_text)


if __name__ == "__main__":
    main()
